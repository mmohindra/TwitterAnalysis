{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RunModel.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHKbvgbE_GB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dfe1d72-314d-41d3-e1cb-86f1eb0ee7c3"
      },
      "source": [
        "# setup\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "required = {'spacy', 'scikit-learn', 'numpy', \n",
        "            'pandas', 'torch', 'matplotlib'}\n",
        "            \n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "\n",
        "\n",
        "from spacy.lang.en import English\n",
        "en = English()\n",
        "#!python -m spacy download en_core_web_md\n",
        "#import en_core_web_md\n",
        "#nlp = en_core_web_md.load()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# this will set the device on which to train\n",
        "#device = torch.device(\"cpu\")\n",
        "# if using collab, set your runtime to use GPU and use the line below\n",
        "device = torch.device(\"cuda:0\")\n",
        "#Ensure GPU active\n",
        "print('GPU active', torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU active True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yRWhsXrMIKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27f122d5-ef0d-4b51-9680-a35b14f96342"
      },
      "source": [
        "print('GPU active', torch.cuda.is_available())\n",
        "if not torch.cuda.is_available():\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU active True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK5RNsYxNBGq",
        "colab_type": "text"
      },
      "source": [
        "Tokenizer and some helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGGRHy6eIDVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f77021b6-cc2a-4893-d7a0-77124de2e381"
      },
      "source": [
        "\n",
        "#remove punctuation and URLs, and stopwords\n",
        "def tokenize(text, model=en, nostopwds=True):\n",
        "   \n",
        "    tokenlist = []\n",
        "    doc = model(text)\n",
        "    ent = ''\n",
        "    for t in doc:\n",
        "      \n",
        "      if nostopwds and t.is_stop:\n",
        "        #print(t.text)\n",
        "        continue\n",
        "      if t.like_url:\n",
        "        tokenlist.append('URL')\n",
        "        continue\n",
        "      if not t.is_alpha:\n",
        "        continue      \n",
        "      tokenlist.append(t.lower_)\n",
        "    return tokenlist\n",
        "text= \"Lol, th? oh @you:all got &amp friend for the d?g ?.. U.S. I'm at a  buffet... Cine there got amore wat... \"\n",
        "print(tokenize(text,nostopwds=False))\n",
        "\n",
        "\n",
        "def doc_to_index(docs, vocab):\n",
        "    # transform docs into series of indices\n",
        "    docs_idxs = []\n",
        "    for d in docs:\n",
        "        w_idxs = []\n",
        "        for w in d:\n",
        "            if w in vocab:\n",
        "                w_idxs.append(vocab[w])\n",
        "            else:\n",
        "                # unknown token = 1\n",
        "                w_idxs.append(1)\n",
        "        docs_idxs.append(w_idxs)\n",
        "    return(docs_idxs)\n",
        "\n",
        "def pad_sequence(seqs, seq_len=300):\n",
        "    # function for adding padding to ensure all seq same length\n",
        "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        if len(seq) != 0:\n",
        "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
        "    return features"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lol', 'th', 'oh', 'all', 'got', 'amp', 'friend', 'for', 'the', 'i', 'at', 'a', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQYuP_3qNFt2",
        "colab_type": "text"
      },
      "source": [
        "copying Sentiment class, could not get import to work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9uNDXM9wPtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Ideally should be a separate class import module but could not get import to work so for now adding directly \n",
        "class SentimentNet(nn.Module):\n",
        "    # sentiment classifier with single LSTM layer + Fully-connected layer, sigmoid activation and dropout\n",
        "    # adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
        "    def __init__(self,\n",
        "                 weight_matrix=None,\n",
        "                 vocab_size=1000, \n",
        "                 output_size=1,  \n",
        "                 hidden_dim=512,\n",
        "                 embedding_dim=400, \n",
        "                 n_layers=2, \n",
        "                 dropout_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        # size of the output, in this case it's one input to one output\n",
        "        self.output_size = output_size\n",
        "        # number of layers (default 2) one LSTM layer, one fully-connected layer\n",
        "        self.n_layers = n_layers\n",
        "        # dimensions of our hidden state, what is passed from one time point to the next\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # initialize the representation to pass to the LSTM\n",
        "        self.embedding, embedding_dim = self.init_embedding(\n",
        "            vocab_size, \n",
        "            embedding_dim, \n",
        "            weight_matrix)\n",
        "        # LSTM layer, where the magic happens\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=dropout_prob, batch_first=True)\n",
        "        # dropout, similar to regularization\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # sigmoid activiation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        # forward pass of the network\n",
        "        batch_size = x.size(0)\n",
        "        # transform input\n",
        "        embeds = self.embedding(x)\n",
        "        # run input embedding + hidden state through model\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        # reshape\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # dropout certain pct of connections\n",
        "        out = self.dropout(lstm_out)\n",
        "        # fully connected layer\n",
        "        out = self.fc(out)\n",
        "        # activation function\n",
        "        out = self.sigmoid(out)\n",
        "        # reshape\n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        # return the output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_embedding(self, vocab_size, embedding_dim, weight_matrix):\n",
        "        # initializes the embedding\n",
        "        if weight_matrix is None:\n",
        "            if vocab_size is None:\n",
        "                raise ValueError('If no weight matrix, need a vocab size')\n",
        "            # if embedding is a size, initialize trainable\n",
        "            return(nn.Embedding(vocab_size, embedding_dim),\n",
        "                   embedding_dim)\n",
        "        else:\n",
        "            # otherwise use matrix as pretrained\n",
        "            weights = torch.FloatTensor(weight_matrix)\n",
        "            return(nn.Embedding.from_pretrained(weights),\n",
        "                  weights.shape[1])\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # initializes the hidden state\n",
        "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
        "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
        "        return hidden"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvwVUGmbNLz5",
        "colab_type": "text"
      },
      "source": [
        "#Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffuf1CueTxv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from ModelLSTM import SentimentNet\n",
        "def initialize_model():\n",
        "  print('Loading model')\n",
        "  model = torch.load('trained_lstm.pt')\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7pbJjupsnNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "8424b896-a981-42ed-ce60-de13916b0235"
      },
      "source": [
        "\n",
        "model = initialize_model()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfBMsMhCHUzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5f6f85-c83d-4c59-999f-eb0079a760f6"
      },
      "source": [
        "if model is not None:\n",
        "  print('Model on cuda?',next(model.parameters()).is_cuda)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model on cuda? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JO14rz1Omr6",
        "colab_type": "text"
      },
      "source": [
        "#Load model vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhRJfMmTOpX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('vocab_dict_10k.pkl', 'rb') as f:\n",
        "    vocab = pickle.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5IQaChnNRlE",
        "colab_type": "text"
      },
      "source": [
        "Loading sample data for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUImK3ZYuO45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a0d041a9-a582-4fb7-cc3e-e1a79ad9b963"
      },
      "source": [
        "##prepare tensor for running thru model\n",
        "with open('clean_tweets_5k.pkl', 'rb') as f:\n",
        "    df_tweet = pickle.load(f)\n",
        "\n",
        "df_tweet['Target'] = df_tweet['Target'].replace(4,1)\n",
        "print(df_tweet['Target'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    2508\n",
            "0    2492\n",
            "Name: Target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diIu_AE6qHe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##prepare tensor for running thru model for large dataset\n",
        "\n",
        "def prepare_data(vocab, df_tweet):\n",
        "\n",
        "  parsed_text = [tokenize(str(d),nostopwds=False) for d in df_tweet]\n",
        "  # idx  has indexes of words in vocab dictionary\n",
        "  #padded pads to 200 length each word sentence if needed\n",
        "  idx = doc_to_index(parsed_text, vocab)\n",
        "  padded_text = pad_sequence(idx)\n",
        "  print('padded text',padded_text.shape)\n",
        "  tensor_data = torch.from_numpy(padded_text)\n",
        "  return (tensor_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474wtCkINYMW",
        "colab_type": "text"
      },
      "source": [
        "Predict_Multiple Method that can be used in a batch usecase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WlCd8BlNanY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##This method can take multiple tweets and create output\n",
        "def predict_multiple(model,vocab, df_tweet):\n",
        "    # utility for assessing accuracy\n",
        "    tensor_data = prepare_data(vocab, df_tweet)\n",
        "    batch_size = tensor_data.size(0)   # can improve logic to do batch_size using all length\n",
        "    text_loader = DataLoader(tensor_data, shuffle=True, batch_size=batch_size,\n",
        "                         drop_last=True) \n",
        "    for i,inputs in enumerate(text_loader):\n",
        "        print(i, type(inputs), len(text_loader))\n",
        "    print('batchsize', batch_size)\n",
        "    model.eval()\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    print('Start Predicting')\n",
        "    for i,inputs in enumerate(text_loader):\n",
        "        h = tuple([each.data for each in h])\n",
        "        inputs = inputs.to(device)\n",
        "        output, h = model(inputs, h)\n",
        "        # takes output, rounds to 0/1\n",
        "        pred = torch.round(output.squeeze())\n",
        "        print(pred)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht2gLf53NnvP",
        "colab_type": "text"
      },
      "source": [
        "Generate sample data for testing on multiple tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFG4c4ZN27P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "d5c9e118-d96f-42ac-fb15-75e8faacde8d"
      },
      "source": [
        "idx = np.random.randint(len(df_tweet))\n",
        "\n",
        "sample_tweet = df_tweet.iloc[idx:idx+5]['text']\n",
        "sample_tweet\n",
        "#predict_multiple(model,vocab, sample_tweet)\n",
        "#prepare_data(df_tweet.iloc[5:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26285    @aaronmartirano  so easy to appreciate people ...\n",
              "29532    Interested in having me write about fashion/sh...\n",
              "32041                        @unkleEL thanks 4 the follow \n",
              "16605    @kattysukamto kesel gw... Konflik ktr is sucks...\n",
              "22020    Not feeling great today....as it should have b...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF2RMfgqPB3Y",
        "colab_type": "text"
      },
      "source": [
        "#Do prediction on multiple tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9sQTZTDKQnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ec9caf9e-0a22-4510-a542-fa228b88e1be"
      },
      "source": [
        "predict_multiple(model, vocab, sample_tweet)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padded text (5, 300)\n",
            "0 <class 'torch.Tensor'> 1\n",
            "batchsize 5\n",
            "Start Predicting\n",
            "tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<RoundBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uErIpBfNt-R",
        "colab_type": "text"
      },
      "source": [
        "Method for predicting on a single tweet to be used from a web api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "864xpY5JeiBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model,vocab, text, seqlen=300):\n",
        "  parsed_text = tokenize(str(text),nostopwds=False)\n",
        " \n",
        "  ##create word index\n",
        "  w_idx = []\n",
        "  for w in parsed_text:\n",
        "    if w in vocab:\n",
        "      w_idx.append(vocab[w])\n",
        "    else:\n",
        "      # unknown token = 1\n",
        "      w_idx.append(1)\n",
        "       \n",
        "  #print(parsed_text,'\\n',w_idx)\n",
        "  padded_text = np.zeros((1,seqlen),dtype=int)\n",
        "  padded_text[0,-len(w_idx):] = np.array(w_idx)[:seqlen]\n",
        "  #padded_text = pad_sequence(w_idx)\n",
        "  #print('padded_text', padded_text.shape)\n",
        "  tensor_data = torch.from_numpy(padded_text)\n",
        "  batch_size = tensor_data.size(0)\n",
        "  print('batchsize',batch_size)\n",
        "\n",
        "  model.eval()\n",
        "  h = model.init_hidden(batch_size)\n",
        "  tensor_data = tensor_data.to(device)\n",
        "  output, h = model(tensor_data, h)\n",
        "  # takes output, rounds to 0/1\n",
        "  pred = torch.round(output.squeeze())\n",
        "  if (pred.item() == 0):\n",
        "     print('prediction negative sentiment')\n",
        "  else:\n",
        "    print('prediction positive sentiment')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5mUr1dwO7zS",
        "colab_type": "text"
      },
      "source": [
        "#Do prediction on 1 tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl4EHcJrfO-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "683f853f-b44a-4291-cc1f-b41370cb6f19"
      },
      "source": [
        "\n",
        "##Generate test data using existing tweets\n",
        "idx = np.random.randint(len(df_tweet))\n",
        "test_tweet = df_tweet.iloc[idx]['text']\n",
        "print(test_tweet,df_tweet.iloc[idx]['Target'] )\n",
        "predict(model,vocab, test_tweet)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Damn Friday Mondays  0\n",
            "batchsize 1\n",
            "prediction negative sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}